{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from db_queries import get_ids, get_outputs, get_location_metadata, get_population, get_covariate_estimates\n",
    "from get_draws.api import get_draws\n",
    "import scipy.stats \n",
    "import scipy.integrate as integrate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create generalized/customizable functions that can be used for Large Scale Food Fortification multiplication models with dichotomous outcomes (zinc, vitamin A, folic acid). The outcomes (DALYs averted) generated by this notebook assume the following:\n",
    "\n",
    "- Complete scale-up achieved between starting baseline and alternative scenario coverage (med/high/low levels), defined according to the proportion of the population that eats industrially produced vehicles. This notebook does NOT currently consider the additional coverage over time in the alternative scenario defined according to the proportion of the population that eats the vehicle at all (due to campaign to convince additional individuals to eat fortified versions of vehicle).\n",
    "- All individuals covered by fortification are assumed to be *effectively* covered. This assumption is not valid based on age- and timing-effects built into the full-scale models. These nutrient-specific effects should be added into the respective mutliplication model for the full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_ids = [163, 214, 205, 190, 189]\n",
    "\n",
    "\"\"\"Note: full set of location IDs is shown below, but subset used here\n",
    "was selected because they are the locations with non-missing coverage data\n",
    "for the nutrient and vehicle of interest (vitamin A/oil)\n",
    "\n",
    "[168, 161, 201, 202, 6, 205, 171, 141, 179, 207, 163, 11, 180, 181,\n",
    "184, 15, 164, 213, 214, 165, 196, 522, 190, 189, 20]\"\"\"\n",
    "\n",
    "ages = [2,3,4,5]\n",
    "sexes = [1,2]\n",
    "\n",
    "index_cols=['location_id','sex_id','age_group_id']\n",
    "\n",
    "# define alternative scenario coverage levels (low, medium, high)\n",
    "    # this parameter represents the proportion of additional coverage achieved in the\n",
    "    # alternative scenario, defined as the difference between the proportion of the population\n",
    "    # that eats the fortified vehicle and the proportion of the population that eats \n",
    "    # the industrially produced vehicle\n",
    "alternative_scenario_coverage_levels = [0.25, 0.5, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitamin A specific -- these should be replaced for other models\n",
    "rei_id = 96\n",
    "cause_ids = [389, 302, 341]\n",
    "nonfatal_causes = [389]\n",
    "nutrient = 'vitamin a'\n",
    "vehicle = 'oil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define no fortification relative risk distribution\n",
    "# vitamin a specific -- this should be replaced for other models\n",
    "\n",
    "from numpy import log\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "# median and 0.975-quantile of lognormal distribution for RR\n",
    "median = 2.22\n",
    "q_975 = 5.26\n",
    "\n",
    "# 0.975-quantile of standard normal distribution (=1.96, approximately)\n",
    "q_975_stdnorm = norm().ppf(0.975)\n",
    "\n",
    "mu = log(median) # mean of normal distribution for log(RR)\n",
    "sigma = (log(q_975) - mu) / q_975_stdnorm # std dev of normal distribution for log(RR)\n",
    "\n",
    "# Frozen lognormal distribution for RR, representing uncertainty in our effect size\n",
    "# (s is the shape parameter)\n",
    "rr_distribution = lognorm(s=sigma, scale=median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fortification_rr_draws_lognormal_dist(mean, std):\n",
    "    \"\"\"This function takes a distribution for the relative risk\n",
    "    for lack of fortification of a particular nutrient and generates\n",
    "    1,000 draws based on that distribution. The data is the duplicated\n",
    "    so that it is the same for each location ID so that it can be easily\n",
    "    used later in the calculations.\"\"\"\n",
    "    data = pd.DataFrame()    \n",
    "    np.random.seed(343)\n",
    "    data['rr'] = np.random.lognormal(mean, std, size=1000)\n",
    "    draws = []\n",
    "    for i in list(range(0,1000)):\n",
    "        draws.append(f'draw_{i}')\n",
    "    data['draws'] = draws\n",
    "    data = pd.DataFrame.pivot_table(data, values='rr', columns='draws').reset_index().drop(columns=['index'])\n",
    "    df = pd.DataFrame(np.repeat(data.values,len(location_ids),axis=0))\n",
    "    df.columns = data.columns\n",
    "    df['location_id'] = location_ids\n",
    "    df = df.set_index('location_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gbd_pafs(rei_id, cause_ids):\n",
    "    \"\"\"This function pulls PAF data from GBD for specified \n",
    "    risk outcome pairs. Note that the risk in this context \n",
    "    will/should be nutrient *deficiencies*, not the lack of \n",
    "    nutrient fortification\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for cause_id in cause_ids:\n",
    "        temp = get_draws(\n",
    "            gbd_id_type=['rei_id', 'cause_id'], \n",
    "            gbd_id=[rei_id, cause_id],\n",
    "            source='burdenator',\n",
    "            measure_id=2, #dalys\n",
    "            metric_id=2, #percent\n",
    "            location_id=location_ids,\n",
    "            year_id=2019,\n",
    "            age_group_id=ages,\n",
    "            sex_id=sexes, \n",
    "            gbd_round_id=6,\n",
    "            status='best',\n",
    "            decomp_step='step5',\n",
    "        )\n",
    "        data = pd.concat([data,temp], ignore_index=True)\n",
    "    data = data.set_index(index_cols + ['cause_id'])\n",
    "    data = data.drop(columns=[c for c in data.columns if 'draw' not in c]).sort_index()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gbd_dalys(cause_ids):\n",
    "    \"\"\"This function pulls dalys for specified cause IDs from GBD\"\"\"\n",
    "    \n",
    "    ylds = get_draws(\n",
    "        gbd_id_type='cause_id', \n",
    "        gbd_id=cause_ids,\n",
    "        source='como',\n",
    "        measure_id=3,\n",
    "        metric_id=3, # only available as rate\n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes, \n",
    "        gbd_round_id=6,\n",
    "        status='best',\n",
    "        decomp_step='step5',\n",
    "    ).set_index(index_cols + ['cause_id'])\n",
    "    ylds = ylds.drop(columns=[c for c in ylds.columns if 'draw' not in c])\n",
    "    pop = get_population(\n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes,\n",
    "        gbd_round_id=6,\n",
    "        decomp_step='step4').set_index(index_cols)\n",
    "    for i in list(range(0,1000)):\n",
    "        ylds[f'draw_{i}'] = ylds[f'draw_{i}'] * pop['population']\n",
    "    ylls = get_draws(\n",
    "        gbd_id_type='cause_id', \n",
    "        gbd_id=cause_ids,\n",
    "        source='codcorrect',\n",
    "        measure_id=4,\n",
    "        metric_id=1, \n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes, \n",
    "        gbd_round_id=6,\n",
    "        status='latest',\n",
    "        decomp_step='step5',\n",
    "    ).set_index(index_cols + ['cause_id']).replace(np.nan, 0)\n",
    "    ylls= ylls.drop(columns=[c for c in ylls.columns if 'draw' not in c])\n",
    "    for nf in nonfatal_causes:\n",
    "        nonfatal = ylls.groupby(index_cols).sum()\n",
    "        nonfatal['cause_id'] = nf\n",
    "        for i in list(range(0,1000)):\n",
    "            nonfatal[f'draw_{i}'] = 0\n",
    "    ylls = pd.concat([ylls.reset_index(), nonfatal.reset_index()]).set_index(index_cols + ['cause_id'])\n",
    "    \n",
    "    dalys = ylls + ylds\n",
    "    return dalys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coverage_data(nutrient, vehicle):\n",
    "    data = pd.read_csv('/ihme/homes/alibow/notebooks/vivarium_data_analysis/pre_processing/lsff_project/data_prep/outputs/LSFF_extraction_clean_data_rich_locations_01_11_2021.csv')\n",
    "    alpha = (data.loc[data.vehicle == vehicle]\n",
    "             .loc[data.nutrient == nutrient]\n",
    "             .loc[data.value_description == 'percent of population eating fortified vehicle'])\n",
    "    alpha_star = (data.loc[data.vehicle == vehicle]\n",
    "                  .loc[data.value_description == 'percent of population eating industrially produced vehicle'])\n",
    "\n",
    "    \n",
    "    # generate draws\n",
    "    \"\"\"This currently relies on two major assumptions:\n",
    "    1. Truncated normal distribution\n",
    "    2. The same percentile from the eats_fortified and eats_fortifiable distributions sampled for each draw\n",
    "    \n",
    "    Assumption number two is likely overly restrictive, but was chosen such that eats_fortified will \n",
    "    always be less than eats_fortifiable at the draw level (this is consistent with methodology described\n",
    "    in 2017 concept model, but is achieved by setting the same random seed to sample each of these\n",
    "    parameters)\"\"\"\n",
    "      \n",
    "    for data in [alpha, alpha_star]:\n",
    "              \n",
    "        data['value_std'] = (data.value_975_percentile - data.value_025_percentile) / 2 / 1.96\n",
    "        data['a'] = (data.value_025_percentile - data.value_mean) / data.value_std\n",
    "        data['b'] = (data.value_975_percentile - data.value_mean) / data.value_std       \n",
    "        np.random.seed(1246)\n",
    "        for i in list(range(0,1000)):\n",
    "            data[f'draw_{i}'] = scipy.stats.truncnorm.rvs(data.a, data.b, data.value_mean, data.value_std) / 100\n",
    "            \n",
    "    alpha = (alpha.set_index('location_id')\n",
    "         .drop(columns=[c for c in alpha.columns if 'draw' not in c and c != 'location_id']))\n",
    "    alpha_star = (alpha_star.set_index('location_id')\n",
    "         .drop(columns=[c for c in alpha_star.columns if 'draw' not in c and c != 'location_id']))\n",
    "    alpha_star_low = (alpha_star - alpha) * alternative_scenario_coverage_levels[0] + alpha\n",
    "    alpha_star_low['coverage_level'] = 'low'\n",
    "    alpha_star_med = (alpha_star - alpha) * alternative_scenario_coverage_levels[1] + alpha\n",
    "    alpha_star_med['coverage_level'] = 'medium'\n",
    "    alpha_star_high = (alpha_star - alpha) * alternative_scenario_coverage_levels[2] + alpha\n",
    "    alpha_star_high['coverage_level'] = 'high'\n",
    "    \n",
    "    alpha_star = pd.concat([alpha_star_low.reset_index(), \n",
    "                            alpha_star_med.reset_index(), \n",
    "                            alpha_star_high.reset_index()], \n",
    "                           ignore_index=True)\n",
    "    alpha_star = alpha_star.set_index([c for c in alpha_star.columns if 'draw' not in c])\n",
    "    \n",
    "    p = 1 - alpha\n",
    "    p_star = 1 - alpha_star\n",
    "    \n",
    "    return p, p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fortification_paf(fortification_rrs, p):\n",
    "    \"\"\"This function calculates the population attributable fraction of UNfortified food\n",
    "    on the fortification outcome of interest (outcome defined in the fortification \n",
    "    effect size, which is generally nutrient deficiency)\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "       \n",
    "    fort_paf = ((fortification_rrs - 1) * p) / ((fortification_rrs - 1) * (p + 1))    \n",
    "    return fort_paf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pif(fort_paf, p, p_star):\n",
    "    \"\"\"This function calculates the population impact fraction for UNfortified \n",
    "    foods and nutrient deficiency based on the location-specific coverage\n",
    "    levels of fortified foods; specifically, p (1 - proportion of population\n",
    "    that eats fortified vehicle) and p_start (1 - proportion of population that \n",
    "    eats industrially produced vehicle).\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "    pif = fort_paf * (p - p_star) / p\n",
    "    return pif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daly_reduction_by_cause(pif, pafs, dalys):\n",
    "    \"\"\"This functionc calculates the population impact fraction for UNfortified \n",
    "    food and DALYs due to specific causes as well as the total number of DALYs\n",
    "    averted by cause, sex, and age\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for level in ['low','medium','high']:\n",
    "        pif_level = (pif.reset_index()\n",
    "                     .loc[pif.reset_index().coverage_level == level]\n",
    "                     .drop(columns='coverage_level')\n",
    "                     .set_index('location_id'))\n",
    "        pif_dalys = pif_level * pafs\n",
    "        pif_dalys['measure'] = 'pif'\n",
    "        dalys_reduction = pif_dalys * dalys\n",
    "        dalys_reduction['measure'] = 'dalys averted'\n",
    "        dalys_reduction_overall = dalys_reduction.reset_index().groupby(index_cols + ['measure']).sum().reset_index()\n",
    "        dalys_reduction_overall['cause_id'] = 294\n",
    "        data = (pd.concat([pif_dalys.reset_index(), dalys_reduction.reset_index(), dalys_reduction_overall], ignore_index=True))\n",
    "        data['coverage_level'] = level\n",
    "        data = data.set_index(index_cols + ['measure','cause_id','coverage_level']).dropna().sort_index()\n",
    "        df = pd.concat([df,data])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihme/code/central_comp/miniconda_svc-ccomp/envs/v104/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fort_deficiency_rrs = generate_fortification_rr_draws_lognormal_dist(mu, sigma)\n",
    "gbd_pafs = pull_gbd_pafs(rei_id, cause_ids)\n",
    "dalys = pull_gbd_dalys(cause_ids)\n",
    "p, p_star = load_coverage_data(nutrient, vehicle)\n",
    "fort_deficiency_paf = calculate_fortification_paf(fort_deficiency_rrs, p)\n",
    "fort_deficiency_pif = calculate_pif(fort_deficiency_paf, p, p_star)\n",
    "fort_daly_reduction = calculate_daly_reduction_by_cause(fort_deficiency_pif, gbd_pafs, dalys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and make sure that there are only negative dalys averted for execpted draws\n",
    "    # (draws with RR for fortification < 1 and draws with negative GBD PAFs)\n",
    "\n",
    "in_neg_draws = np.concatenate([pd.DataFrame(fort_deficiency_rrs.stack()).loc[pd.DataFrame(fort_deficiency_rrs.stack())[0] < 1].reset_index()['draws'].unique(),\n",
    "            pd.DataFrame(gbd_pafs.stack()).loc[pd.DataFrame(gbd_pafs.stack())[0] < 0].reset_index()['level_4'].unique()])\n",
    "\n",
    "out_neg_draws = pd.DataFrame(fort_daly_reduction.stack()).reset_index().rename(columns={'level_6':'draw',0:'val'})\n",
    "out_neg_draws = out_neg_draws.loc[out_neg_draws.val < 0]\n",
    "\n",
    "assert len([c for c in out_neg_draws.draw.unique() if c not in in_neg_draws]) == 0, \"Error: unexpected negative values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_id</th>\n",
       "      <th>measure</th>\n",
       "      <th>cause_id</th>\n",
       "      <th>coverage_level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">163</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">294</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>69633.656512</td>\n",
       "      <td>21182.516951</td>\n",
       "      <td>3354.346363</td>\n",
       "      <td>32986.620181</td>\n",
       "      <td>68485.906767</td>\n",
       "      <td>117659.853124</td>\n",
       "      <td>148071.266736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>23211.218837</td>\n",
       "      <td>7060.838984</td>\n",
       "      <td>1118.115454</td>\n",
       "      <td>10995.540060</td>\n",
       "      <td>22828.635589</td>\n",
       "      <td>39219.951041</td>\n",
       "      <td>49357.088912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>46422.437675</td>\n",
       "      <td>14121.677967</td>\n",
       "      <td>2236.230908</td>\n",
       "      <td>21991.080120</td>\n",
       "      <td>45657.271178</td>\n",
       "      <td>78439.902083</td>\n",
       "      <td>98714.177824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">189</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">294</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>7545.163174</td>\n",
       "      <td>3684.105084</td>\n",
       "      <td>-2475.625731</td>\n",
       "      <td>2321.637806</td>\n",
       "      <td>6963.271813</td>\n",
       "      <td>16551.280176</td>\n",
       "      <td>26894.770469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2515.054391</td>\n",
       "      <td>1228.035028</td>\n",
       "      <td>-825.208577</td>\n",
       "      <td>773.879269</td>\n",
       "      <td>2321.090604</td>\n",
       "      <td>5517.093392</td>\n",
       "      <td>8964.923490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5030.108782</td>\n",
       "      <td>2456.070056</td>\n",
       "      <td>-1650.417154</td>\n",
       "      <td>1547.758538</td>\n",
       "      <td>4642.181209</td>\n",
       "      <td>11034.186784</td>\n",
       "      <td>17929.846980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">190</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">294</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>7257.985482</td>\n",
       "      <td>4130.561112</td>\n",
       "      <td>-2191.300189</td>\n",
       "      <td>1451.733771</td>\n",
       "      <td>6406.459741</td>\n",
       "      <td>17870.334414</td>\n",
       "      <td>29822.104818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2419.328494</td>\n",
       "      <td>1376.853704</td>\n",
       "      <td>-730.433396</td>\n",
       "      <td>483.911257</td>\n",
       "      <td>2135.486580</td>\n",
       "      <td>5956.778138</td>\n",
       "      <td>9940.701606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>4838.656988</td>\n",
       "      <td>2753.707408</td>\n",
       "      <td>-1460.866793</td>\n",
       "      <td>967.822514</td>\n",
       "      <td>4270.973160</td>\n",
       "      <td>11913.556276</td>\n",
       "      <td>19881.403212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">205</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">294</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">214</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">294</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>21582.833833</td>\n",
       "      <td>10287.101631</td>\n",
       "      <td>-4609.538526</td>\n",
       "      <td>4791.562258</td>\n",
       "      <td>20166.618229</td>\n",
       "      <td>43795.409438</td>\n",
       "      <td>71107.291422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>7194.277944</td>\n",
       "      <td>3429.033877</td>\n",
       "      <td>-1536.512842</td>\n",
       "      <td>1597.187419</td>\n",
       "      <td>6722.206076</td>\n",
       "      <td>14598.469813</td>\n",
       "      <td>23702.430474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>14388.555888</td>\n",
       "      <td>6858.067754</td>\n",
       "      <td>-3073.025684</td>\n",
       "      <td>3194.374838</td>\n",
       "      <td>13444.412153</td>\n",
       "      <td>29196.939625</td>\n",
       "      <td>47404.860948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    count          mean  \\\n",
       "location_id measure       cause_id coverage_level                         \n",
       "163         dalys averted 294      high            1000.0  69633.656512   \n",
       "                                   low             1000.0  23211.218837   \n",
       "                                   medium          1000.0  46422.437675   \n",
       "189         dalys averted 294      high            1000.0   7545.163174   \n",
       "                                   low             1000.0   2515.054391   \n",
       "                                   medium          1000.0   5030.108782   \n",
       "190         dalys averted 294      high            1000.0   7257.985482   \n",
       "                                   low             1000.0   2419.328494   \n",
       "                                   medium          1000.0   4838.656988   \n",
       "205         dalys averted 294      high            1000.0      0.000000   \n",
       "                                   low             1000.0      0.000000   \n",
       "                                   medium          1000.0      0.000000   \n",
       "214         dalys averted 294      high            1000.0  21582.833833   \n",
       "                                   low             1000.0   7194.277944   \n",
       "                                   medium          1000.0  14388.555888   \n",
       "\n",
       "                                                            std          min  \\\n",
       "location_id measure       cause_id coverage_level                              \n",
       "163         dalys averted 294      high            21182.516951  3354.346363   \n",
       "                                   low              7060.838984  1118.115454   \n",
       "                                   medium          14121.677967  2236.230908   \n",
       "189         dalys averted 294      high             3684.105084 -2475.625731   \n",
       "                                   low              1228.035028  -825.208577   \n",
       "                                   medium           2456.070056 -1650.417154   \n",
       "190         dalys averted 294      high             4130.561112 -2191.300189   \n",
       "                                   low              1376.853704  -730.433396   \n",
       "                                   medium           2753.707408 -1460.866793   \n",
       "205         dalys averted 294      high                0.000000     0.000000   \n",
       "                                   low                 0.000000     0.000000   \n",
       "                                   medium              0.000000     0.000000   \n",
       "214         dalys averted 294      high            10287.101631 -4609.538526   \n",
       "                                   low              3429.033877 -1536.512842   \n",
       "                                   medium           6858.067754 -3073.025684   \n",
       "\n",
       "                                                           2.5%           50%  \\\n",
       "location_id measure       cause_id coverage_level                               \n",
       "163         dalys averted 294      high            32986.620181  68485.906767   \n",
       "                                   low             10995.540060  22828.635589   \n",
       "                                   medium          21991.080120  45657.271178   \n",
       "189         dalys averted 294      high             2321.637806   6963.271813   \n",
       "                                   low               773.879269   2321.090604   \n",
       "                                   medium           1547.758538   4642.181209   \n",
       "190         dalys averted 294      high             1451.733771   6406.459741   \n",
       "                                   low               483.911257   2135.486580   \n",
       "                                   medium            967.822514   4270.973160   \n",
       "205         dalys averted 294      high                0.000000      0.000000   \n",
       "                                   low                 0.000000      0.000000   \n",
       "                                   medium              0.000000      0.000000   \n",
       "214         dalys averted 294      high             4791.562258  20166.618229   \n",
       "                                   low              1597.187419   6722.206076   \n",
       "                                   medium           3194.374838  13444.412153   \n",
       "\n",
       "                                                           97.5%  \\\n",
       "location_id measure       cause_id coverage_level                  \n",
       "163         dalys averted 294      high            117659.853124   \n",
       "                                   low              39219.951041   \n",
       "                                   medium           78439.902083   \n",
       "189         dalys averted 294      high             16551.280176   \n",
       "                                   low               5517.093392   \n",
       "                                   medium           11034.186784   \n",
       "190         dalys averted 294      high             17870.334414   \n",
       "                                   low               5956.778138   \n",
       "                                   medium           11913.556276   \n",
       "205         dalys averted 294      high                 0.000000   \n",
       "                                   low                  0.000000   \n",
       "                                   medium               0.000000   \n",
       "214         dalys averted 294      high             43795.409438   \n",
       "                                   low              14598.469813   \n",
       "                                   medium           29196.939625   \n",
       "\n",
       "                                                             max  \n",
       "location_id measure       cause_id coverage_level                 \n",
       "163         dalys averted 294      high            148071.266736  \n",
       "                                   low              49357.088912  \n",
       "                                   medium           98714.177824  \n",
       "189         dalys averted 294      high             26894.770469  \n",
       "                                   low               8964.923490  \n",
       "                                   medium           17929.846980  \n",
       "190         dalys averted 294      high             29822.104818  \n",
       "                                   low               9940.701606  \n",
       "                                   medium           19881.403212  \n",
       "205         dalys averted 294      high                 0.000000  \n",
       "                                   low                  0.000000  \n",
       "                                   medium               0.000000  \n",
       "214         dalys averted 294      high             71107.291422  \n",
       "                                   low              23702.430474  \n",
       "                                   medium           47404.860948  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fort_daly_reduction_by_location = fort_daly_reduction.groupby(['location_id','measure','cause_id','coverage_level']).sum().reset_index()\n",
    "fort_daly_reduction_by_location = (fort_daly_reduction_by_location\n",
    "                                   .loc[fort_daly_reduction_by_location.measure=='dalys averted']\n",
    "                                   .loc[fort_daly_reduction_by_location.cause_id==294])\n",
    "fort_daly_reduction_by_location = (fort_daly_reduction_by_location\n",
    "                                   .set_index(['location_id','measure','cause_id','coverage_level'])\n",
    "                                   .apply(pd.DataFrame.describe, percentiles=[0.025,0.975], axis=1))\n",
    "    \n",
    "fort_daly_reduction_by_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
