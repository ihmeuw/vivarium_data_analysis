{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from db_queries import get_ids, get_outputs, get_location_metadata, get_population, get_covariate_estimates\n",
    "from get_draws.api import get_draws\n",
    "import scipy.stats \n",
    "import scipy.integrate as integrate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create generalized/customizable functions that can be used for Large Scale Food Fortification multiplication models with dichotomous outcomes (zinc, vitamin A, folic acid). The outcomes (DALYs averted) generated by this notebook assume the following:\n",
    "\n",
    "- Complete scale-up achieved between starting baseline and alternative scenario coverage (med/high/low levels), defined according to the proportion of the population that eats industrially produced vehicles. This notebook does NOT currently consider the additional coverage over time in the alternative scenario defined according to the proportion of the population that eats the vehicle at all (due to campaign to convince additional individuals to eat fortified versions of vehicle).\n",
    "- All individuals covered by fortification are assumed to be *effectively* covered. This assumption is not valid based on age- and timing-effects built into the full-scale models. These nutrient-specific effects should be added into the respective mutliplication model for the full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_ids = [163, 214, 205, 190, 189]\n",
    "\n",
    "\"\"\"Note: full set of location IDs is shown below, but subset used here\n",
    "was selected because they are the locations with non-missing coverage data\n",
    "for the nutrient and vehicle of interest (vitamin A/oil)\n",
    "\n",
    "[168, 161, 201, 202, 6, 205, 171, 141, 179, 207, 163, 11, 180, 181,\n",
    "184, 15, 164, 213, 214, 165, 196, 522, 190, 189, 20]\"\"\"\n",
    "\n",
    "ages = [2,3,4,5]\n",
    "sexes = [1,2]\n",
    "\n",
    "index_cols=['location_id','sex_id','age_group_id']\n",
    "\n",
    "# define alternative scenario coverage levels (low, medium, high)\n",
    "    # this parameter represents the proportion of additional coverage achieved in the\n",
    "    # alternative scenario, defined as the difference between the proportion of the population\n",
    "    # that eats the fortified vehicle and the proportion of the population that eats \n",
    "    # the industrially produced vehicle\n",
    "alternative_scenario_coverage_levels = [0.25, 0.5, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vitamin A specific -- these should be replaced for other models\n",
    "rei_id = 96\n",
    "cause_ids = [389, 302, 341]\n",
    "nonfatal_causes = [389]\n",
    "nutrient = 'vitamin a'\n",
    "vehicle = 'oil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define no fortification relative risk distribution\n",
    "# vitamin a specific -- this should be replaced for other models\n",
    "\n",
    "from numpy import log\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "# median and 0.975-quantile of lognormal distribution for RR\n",
    "median = 2.22\n",
    "q_975 = 5.26\n",
    "\n",
    "# 0.975-quantile of standard normal distribution (=1.96, approximately)\n",
    "q_975_stdnorm = norm().ppf(0.975)\n",
    "\n",
    "mu = log(median) # mean of normal distribution for log(RR)\n",
    "sigma = (log(q_975) - mu) / q_975_stdnorm # std dev of normal distribution for log(RR)\n",
    "\n",
    "# Frozen lognormal distribution for RR, representing uncertainty in our effect size\n",
    "# (s is the shape parameter)\n",
    "rr_distribution = lognorm(s=sigma, scale=median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fortification_rr_draws_lognormal_dist(mean, std):\n",
    "    \"\"\"This function takes a distribution for the relative risk\n",
    "    for lack of fortification of a particular nutrient and generates\n",
    "    1,000 draws based on that distribution. The data is the duplicated\n",
    "    so that it is the same for each location ID so that it can be easily\n",
    "    used later in the calculations.\"\"\"\n",
    "    data = pd.DataFrame()    \n",
    "    np.random.seed(343)\n",
    "    data['rr'] = np.random.lognormal(mean, std, size=1000)\n",
    "    draws = []\n",
    "    for i in list(range(0,1000)):\n",
    "        draws.append(f'draw_{i}')\n",
    "    data['draws'] = draws\n",
    "    data = pd.DataFrame.pivot_table(data, values='rr', columns='draws').reset_index().drop(columns=['index'])\n",
    "    df = pd.DataFrame(np.repeat(data.values,len(location_ids),axis=0))\n",
    "    df.columns = data.columns\n",
    "    df['location_id'] = location_ids\n",
    "    df = df.set_index('location_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gbd_pafs(rei_id, cause_ids):\n",
    "    \"\"\"This function pulls PAF data from GBD for specified \n",
    "    risk outcome pairs. Note that the risk in this context \n",
    "    will/should be nutrient *deficiencies*, not the lack of \n",
    "    nutrient fortification\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for cause_id in cause_ids:\n",
    "        temp = get_draws(\n",
    "            gbd_id_type=['rei_id', 'cause_id'], \n",
    "            gbd_id=[rei_id, cause_id],\n",
    "            source='burdenator',\n",
    "            measure_id=2, #dalys\n",
    "            metric_id=2, #percent\n",
    "            location_id=location_ids,\n",
    "            year_id=2019,\n",
    "            age_group_id=ages,\n",
    "            sex_id=sexes, \n",
    "            gbd_round_id=6,\n",
    "            status='best',\n",
    "            decomp_step='step5',\n",
    "        )\n",
    "        data = pd.concat([data,temp], ignore_index=True)\n",
    "    data = data.set_index(index_cols + ['cause_id'])\n",
    "    data = data.drop(columns=[c for c in data.columns if 'draw' not in c]).sort_index()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_gbd_dalys(cause_ids):\n",
    "    \"\"\"This function pulls dalys for specified cause IDs from GBD\"\"\"\n",
    "    \n",
    "    ylds = get_draws(\n",
    "        gbd_id_type='cause_id', \n",
    "        gbd_id=cause_ids,\n",
    "        source='como',\n",
    "        measure_id=3,\n",
    "        metric_id=3, # only available as rate\n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes, \n",
    "        gbd_round_id=6,\n",
    "        status='best',\n",
    "        decomp_step='step5',\n",
    "    ).set_index(index_cols + ['cause_id'])\n",
    "    ylds = ylds.drop(columns=[c for c in ylds.columns if 'draw' not in c])\n",
    "    pop = get_population(\n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes,\n",
    "        gbd_round_id=6,\n",
    "        decomp_step='step4').set_index(index_cols)\n",
    "    for i in list(range(0,1000)):\n",
    "        ylds[f'draw_{i}'] = ylds[f'draw_{i}'] * pop['population']\n",
    "    ylls = get_draws(\n",
    "        gbd_id_type='cause_id', \n",
    "        gbd_id=cause_ids,\n",
    "        source='codcorrect',\n",
    "        measure_id=4,\n",
    "        metric_id=1, \n",
    "        location_id=location_ids,\n",
    "        year_id=2019,\n",
    "        age_group_id=ages,\n",
    "        sex_id=sexes, \n",
    "        gbd_round_id=6,\n",
    "        status='latest',\n",
    "        decomp_step='step5',\n",
    "    ).set_index(index_cols + ['cause_id']).replace(np.nan, 0)\n",
    "    ylls= ylls.drop(columns=[c for c in ylls.columns if 'draw' not in c])\n",
    "    for nf in nonfatal_causes:\n",
    "        nonfatal = ylls.groupby(index_cols).sum()\n",
    "        nonfatal['cause_id'] = nf\n",
    "        for i in list(range(0,1000)):\n",
    "            nonfatal[f'draw_{i}'] = 0\n",
    "    ylls = pd.concat([ylls.reset_index(), nonfatal.reset_index()]).set_index(index_cols + ['cause_id'])\n",
    "    \n",
    "    dalys = ylls + ylds\n",
    "    return dalys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coverage_data(nutrient, vehicle):\n",
    "    data = pd.read_csv('/ihme/homes/alibow/notebooks/vivarium_data_analysis/pre_processing/lsff_project/data_prep/outputs/LSFF_extraction_clean_data_rich_locations_01_11_2021.csv')\n",
    "    alpha = (data.loc[data.vehicle == vehicle]\n",
    "             .loc[data.nutrient == nutrient]\n",
    "             .loc[data.value_description == 'percent of population eating fortified vehicle'])\n",
    "    alpha_star = (data.loc[data.vehicle == vehicle]\n",
    "                  .loc[data.value_description == 'percent of population eating industrially produced vehicle'])\n",
    "\n",
    "    \n",
    "    # generate draws\n",
    "    \"\"\"This currently relies on two major assumptions:\n",
    "    1. Normal distribution (not truncated)\n",
    "    2. The same percentile from the eats_fortified and eats_fortifiable distributions sampled for each draw\n",
    "    \n",
    "    Assumption number two is likely overly restrictive, but was chosen such that eats_fortified will \n",
    "    always be less than eats_fortifiable at the draw level (this is consistent with methodology described\n",
    "    in 2017 concept model, but is achieved by setting the same random seed to sample each of these\n",
    "    parameters)\"\"\"\n",
    "    \n",
    "    for data in [alpha, alpha_star]:\n",
    "        np.random.seed(1246)\n",
    "        for i in list(range(0,1000)):\n",
    "            \n",
    "            data[f'draw_{i}'] = np.random.normal(data.value_mean, \n",
    "                                                      (data.value_975_percentile - data.value_025_percentile) / 2 / 1.96) / 100\n",
    "    alpha = (alpha.set_index('location_id')\n",
    "         .drop(columns=[c for c in alpha.columns if 'draw' not in c and c != 'location_id']))\n",
    "    alpha_star = (alpha_star.set_index('location_id')\n",
    "         .drop(columns=[c for c in alpha_star.columns if 'draw' not in c and c != 'location_id']))\n",
    "    alpha_star_low = (alpha_star - alpha) * alternative_scenario_coverage_levels[0] + alpha\n",
    "    alpha_star_low['coverage_level'] = 'low'\n",
    "    alpha_star_med = (alpha_star - alpha) * alternative_scenario_coverage_levels[1] + alpha\n",
    "    alpha_star_med['coverage_level'] = 'medium'\n",
    "    alpha_star_high = (alpha_star - alpha) * alternative_scenario_coverage_levels[2] + alpha\n",
    "    alpha_star_high['coverage_level'] = 'high'\n",
    "    \n",
    "    alpha_star = pd.concat([alpha_star_low.reset_index(), \n",
    "                            alpha_star_med.reset_index(), \n",
    "                            alpha_star_high.reset_index()], \n",
    "                           ignore_index=True)\n",
    "    alpha_star = alpha_star.set_index([c for c in alpha_star.columns if 'draw' not in c])\n",
    "    \n",
    "    p = 1 - alpha\n",
    "    p_star = 1 - alpha_star\n",
    "    \n",
    "    return p, p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fortification_paf(fortification_rrs, p):\n",
    "    \"\"\"This function calculates the population attributable fraction of UNfortified food\n",
    "    on the fortification outcome of interest (outcome defined in the fortification \n",
    "    effect size, which is generally nutrient deficiency)\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "       \n",
    "    fort_paf = ((fortification_rrs - 1) * p) / ((fortification_rrs - 1) * (p + 1))    \n",
    "    return fort_paf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pif(fort_paf, p, p_star):\n",
    "    \"\"\"This function calculates the population impact fraction for UNfortified \n",
    "    foods and nutrient deficiency based on the location-specific coverage\n",
    "    levels of fortified foods; specifically, p (1 - proportion of population\n",
    "    that eats fortified vehicle) and p_start (1 - proportion of population that \n",
    "    eats industrially produced vehicle).\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "    pif = fort_paf * (p - p_star) / p\n",
    "    return pif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daly_reduction_by_cause(pif, pafs, dalys):\n",
    "    \"\"\"This functionc calculates the population impact fraction for UNfortified \n",
    "    food and DALYs due to specific causes as well as the total number of DALYs\n",
    "    averted by cause, sex, and age\n",
    "    \n",
    "    NOTE: this function does not consider age/time lags of fortification effects\n",
    "    (assumes that every individual covered by fortification is effectively covered)\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for level in ['low','medium','high']:\n",
    "        pif_level = (pif.reset_index()\n",
    "                     .loc[pif.reset_index().coverage_level == level]\n",
    "                     .drop(columns='coverage_level')\n",
    "                     .set_index('location_id'))\n",
    "        pif_dalys = pif_level * pafs\n",
    "        pif_dalys['measure'] = 'pif'\n",
    "        dalys_reduction = pif_dalys * dalys\n",
    "        dalys_reduction['measure'] = 'dalys averted'\n",
    "        dalys_reduction_overall = dalys_reduction.reset_index().groupby(index_cols + ['measure']).sum().reset_index()\n",
    "        dalys_reduction_overall['cause_id'] = 'total'\n",
    "        data = (pd.concat([pif_dalys.reset_index(), dalys_reduction.reset_index(), dalys_reduction_overall], ignore_index=True))\n",
    "        data['coverage_level'] = level\n",
    "        data = data.set_index(index_cols + ['measure','cause_id','coverage_level']).dropna().sort_index()\n",
    "        df = pd.concat([df,data])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihme/code/central_comp/miniconda_svc-ccomp/envs/v104/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fort_deficiency_rrs = generate_fortification_rr_draws_lognormal_dist(mu, sigma)\n",
    "gbd_pafs = pull_gbd_pafs(rei_id, cause_ids)\n",
    "dalys = pull_gbd_dalys(cause_ids)\n",
    "p, p_star = load_coverage_data(nutrient, vehicle)\n",
    "fort_deficiency_paf = calculate_fortification_paf(fort_deficiency_rrs, p)\n",
    "fort_deficiency_pif = calculate_pif(fort_deficiency_paf, p, p_star)\n",
    "fort_daly_reduction = calculate_daly_reduction_by_cause(fort_deficiency_pif, gbd_pafs, dalys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and make sure that there are only negative dalys averted for execpted draws\n",
    "    # (draws with RR for fortification < 1 and draws with negative GBD PAFs)\n",
    "\n",
    "in_neg_draws = np.concatenate([pd.DataFrame(fort_deficiency_rrs.stack()).loc[pd.DataFrame(fort_deficiency_rrs.stack())[0] < 1].reset_index()['draws'].unique(),\n",
    "            pd.DataFrame(gbd_pafs.stack()).loc[pd.DataFrame(gbd_pafs.stack())[0] < 0].reset_index()['level_4'].unique()])\n",
    "\n",
    "out_neg_draws = pd.DataFrame(fort_daly_reduction.stack()).reset_index().rename(columns={'level_6':'draw',0:'val'})\n",
    "out_neg_draws = out_neg_draws.loc[out_neg_draws.val < 0]\n",
    "\n",
    "assert len([c for c in out_neg_draws.draw.unique() if c not in in_neg_draws]) == 0, \"Error: unexpected negative values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_id</th>\n",
       "      <th>measure</th>\n",
       "      <th>coverage_level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">163</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>139337.577577</td>\n",
       "      <td>42378.450578</td>\n",
       "      <td>6733.706910</td>\n",
       "      <td>66146.017445</td>\n",
       "      <td>137027.180340</td>\n",
       "      <td>234964.742182</td>\n",
       "      <td>295890.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>46445.859192</td>\n",
       "      <td>14126.150193</td>\n",
       "      <td>2244.568970</td>\n",
       "      <td>22048.672482</td>\n",
       "      <td>45675.726780</td>\n",
       "      <td>78321.580727</td>\n",
       "      <td>98630.023713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>92891.718385</td>\n",
       "      <td>28252.300385</td>\n",
       "      <td>4489.137940</td>\n",
       "      <td>44097.344964</td>\n",
       "      <td>91351.453560</td>\n",
       "      <td>156643.161455</td>\n",
       "      <td>197260.047426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">189</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>15102.861174</td>\n",
       "      <td>7376.391045</td>\n",
       "      <td>-5001.504786</td>\n",
       "      <td>4725.173951</td>\n",
       "      <td>13858.180127</td>\n",
       "      <td>33281.578722</td>\n",
       "      <td>54059.326399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5034.287058</td>\n",
       "      <td>2458.797015</td>\n",
       "      <td>-1667.168262</td>\n",
       "      <td>1575.057984</td>\n",
       "      <td>4619.393376</td>\n",
       "      <td>11093.859574</td>\n",
       "      <td>18019.775466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10068.574116</td>\n",
       "      <td>4917.594030</td>\n",
       "      <td>-3334.336524</td>\n",
       "      <td>3150.115968</td>\n",
       "      <td>9238.786752</td>\n",
       "      <td>22187.719148</td>\n",
       "      <td>36039.550933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">190</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>14522.534447</td>\n",
       "      <td>8267.625156</td>\n",
       "      <td>-4441.768076</td>\n",
       "      <td>2923.005528</td>\n",
       "      <td>12768.716387</td>\n",
       "      <td>35916.145811</td>\n",
       "      <td>60209.663807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>4840.844816</td>\n",
       "      <td>2755.875052</td>\n",
       "      <td>-1480.589359</td>\n",
       "      <td>974.335176</td>\n",
       "      <td>4256.238796</td>\n",
       "      <td>11972.048604</td>\n",
       "      <td>20069.887936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>9681.689632</td>\n",
       "      <td>5511.750104</td>\n",
       "      <td>-2961.178717</td>\n",
       "      <td>1948.670352</td>\n",
       "      <td>8512.477591</td>\n",
       "      <td>23944.097207</td>\n",
       "      <td>40139.775871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">205</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">214</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">dalys averted</th>\n",
       "      <th>high</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>43164.714314</td>\n",
       "      <td>20553.169237</td>\n",
       "      <td>-9244.315251</td>\n",
       "      <td>9090.799887</td>\n",
       "      <td>40719.400286</td>\n",
       "      <td>88436.313013</td>\n",
       "      <td>148137.653306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>14388.238105</td>\n",
       "      <td>6851.056412</td>\n",
       "      <td>-3081.438417</td>\n",
       "      <td>3030.266629</td>\n",
       "      <td>13573.133429</td>\n",
       "      <td>29478.771004</td>\n",
       "      <td>49379.217769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>28776.476209</td>\n",
       "      <td>13702.112824</td>\n",
       "      <td>-6162.876834</td>\n",
       "      <td>6060.533258</td>\n",
       "      <td>27146.266858</td>\n",
       "      <td>58957.542008</td>\n",
       "      <td>98758.435537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           count           mean           std  \\\n",
       "location_id measure       coverage_level                                        \n",
       "163         dalys averted high            1000.0  139337.577577  42378.450578   \n",
       "                          low             1000.0   46445.859192  14126.150193   \n",
       "                          medium          1000.0   92891.718385  28252.300385   \n",
       "189         dalys averted high            1000.0   15102.861174   7376.391045   \n",
       "                          low             1000.0    5034.287058   2458.797015   \n",
       "                          medium          1000.0   10068.574116   4917.594030   \n",
       "190         dalys averted high            1000.0   14522.534447   8267.625156   \n",
       "                          low             1000.0    4840.844816   2755.875052   \n",
       "                          medium          1000.0    9681.689632   5511.750104   \n",
       "205         dalys averted high            1000.0       0.000000      0.000000   \n",
       "                          low             1000.0       0.000000      0.000000   \n",
       "                          medium          1000.0       0.000000      0.000000   \n",
       "214         dalys averted high            1000.0   43164.714314  20553.169237   \n",
       "                          low             1000.0   14388.238105   6851.056412   \n",
       "                          medium          1000.0   28776.476209  13702.112824   \n",
       "\n",
       "                                                  min          2.5%  \\\n",
       "location_id measure       coverage_level                              \n",
       "163         dalys averted high            6733.706910  66146.017445   \n",
       "                          low             2244.568970  22048.672482   \n",
       "                          medium          4489.137940  44097.344964   \n",
       "189         dalys averted high           -5001.504786   4725.173951   \n",
       "                          low            -1667.168262   1575.057984   \n",
       "                          medium         -3334.336524   3150.115968   \n",
       "190         dalys averted high           -4441.768076   2923.005528   \n",
       "                          low            -1480.589359    974.335176   \n",
       "                          medium         -2961.178717   1948.670352   \n",
       "205         dalys averted high               0.000000      0.000000   \n",
       "                          low                0.000000      0.000000   \n",
       "                          medium             0.000000      0.000000   \n",
       "214         dalys averted high           -9244.315251   9090.799887   \n",
       "                          low            -3081.438417   3030.266629   \n",
       "                          medium         -6162.876834   6060.533258   \n",
       "\n",
       "                                                    50%          97.5%  \\\n",
       "location_id measure       coverage_level                                 \n",
       "163         dalys averted high            137027.180340  234964.742182   \n",
       "                          low              45675.726780   78321.580727   \n",
       "                          medium           91351.453560  156643.161455   \n",
       "189         dalys averted high             13858.180127   33281.578722   \n",
       "                          low               4619.393376   11093.859574   \n",
       "                          medium            9238.786752   22187.719148   \n",
       "190         dalys averted high             12768.716387   35916.145811   \n",
       "                          low               4256.238796   11972.048604   \n",
       "                          medium            8512.477591   23944.097207   \n",
       "205         dalys averted high                 0.000000       0.000000   \n",
       "                          low                  0.000000       0.000000   \n",
       "                          medium               0.000000       0.000000   \n",
       "214         dalys averted high             40719.400286   88436.313013   \n",
       "                          low              13573.133429   29478.771004   \n",
       "                          medium           27146.266858   58957.542008   \n",
       "\n",
       "                                                    max  \n",
       "location_id measure       coverage_level                 \n",
       "163         dalys averted high            295890.071139  \n",
       "                          low              98630.023713  \n",
       "                          medium          197260.047426  \n",
       "189         dalys averted high             54059.326399  \n",
       "                          low              18019.775466  \n",
       "                          medium           36039.550933  \n",
       "190         dalys averted high             60209.663807  \n",
       "                          low              20069.887936  \n",
       "                          medium           40139.775871  \n",
       "205         dalys averted high                 0.000000  \n",
       "                          low                  0.000000  \n",
       "                          medium               0.000000  \n",
       "214         dalys averted high            148137.653306  \n",
       "                          low              49379.217769  \n",
       "                          medium           98758.435537  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fort_daly_reduction_by_location = fort_daly_reduction.groupby(['location_id','measure','coverage_level']).sum().reset_index()\n",
    "fort_daly_reduction_by_location = fort_daly_reduction_by_location.loc[fort_daly_reduction_by_location.measure=='dalys averted']\n",
    "fort_daly_reduction_by_location = (fort_daly_reduction_by_location\n",
    "                                   .set_index(['location_id','measure','coverage_level'])\n",
    "                                   .apply(pd.DataFrame.describe, percentiles=[0.025,0.975], axis=1))\n",
    "    \n",
    "fort_daly_reduction_by_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
